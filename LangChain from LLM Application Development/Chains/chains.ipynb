{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates the `use of various chain mechanisms in LangChain` to manage and orchestrate complex workflows involving language models. The primary chain mechanisms covered include `LLMChain, SimpleSequentialChain, SequentialChain, and Router Chain`. These tools enable more effective and structured interactions with AI models by leveraging different chaining techniques. We will cover the following topics:     \n",
    "* LLMChain  \n",
    "* Sequential Chains  \n",
    "  * SimpleSequentialChain  \n",
    "  * SequentialChain  \n",
    "* Router Chain  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1bda7",
   "metadata": {},
   "source": [
    "### LLMChain\n",
    "This section demonstrates how to use LLMChain to manage a single language model prompt and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c21a3",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541eb2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6850e",
   "metadata": {},
   "source": [
    "**Note** To handle the deprecation of LLM models, we use the current date to select the appropriate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336d784-65c2-4a11-8489-b445b1fad177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Model Deprecation\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9987e2",
   "metadata": {},
   "source": [
    "- Ensure you have the required packages installed:\n",
    "```py\n",
    "%pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the pandas library, commonly used for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file named 'Data.csv' into a DataFrame\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a09c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940ce7c",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82adfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain-related libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5fd6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdcdb42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7abc20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#product = 'Queen Size Sheet Set'\n",
    "#chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb34e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LLMChain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\"Royal Linens\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 24, 'total_tokens': 30, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-b7656615-8370-4fb4-9044-81aa2f45b16b-0' usage_metadata={'input_tokens': 24, 'output_tokens': 6, 'total_tokens': 30, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Use .invoke() to get response\n",
    "product = \"Queen Size Sheet Set\"\n",
    "response = chain.invoke({\"product\": product})\n",
    "\n",
    "response\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b03469",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain\n",
    "This section demonstrates how to use SimpleSequentialChain to execute a sequence of chains in a simple linear fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "febee243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import SimpleSequentialChain\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f31aa8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_13772\\3393886043.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain_one = LLMChain(llm=llm, prompt=first_prompt)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# Define the first prompt template\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5d5b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the second prompt template\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c1eb2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SimpleSequentialChain\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dfc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRegal Beddings Co.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRegal Beddings Co. offers luxurious and stylish bedding sets made from high-quality materials to elevate your bedroom décor.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Queen Size Sheet Set',\n",
       " 'output': 'Regal Beddings Co. offers luxurious and stylish bedding sets made from high-quality materials to elevate your bedroom décor.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the chain\n",
    "overall_simple_chain.invoke(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb5bf30",
   "metadata": {},
   "source": [
    "**Explanation output** The above output is an output of a process involving a SimpleSequentialChain in a language model. The output is essentially a log that shows the start and end of a sequential process (SimpleSequentialChain) and includes some processed or generated content related to Regal Beddings Co."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ce18c",
   "metadata": {},
   "source": [
    "### SequentialChain\n",
    "This section demonstrates how to use SequentialChain to manage more complex workflows with multiple chained operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c129ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import SequentialChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4623b",
   "metadata": {},
   "source": [
    "- Translate to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "016187ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# Define the first prompt template (translate to English)\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b2782",
   "metadata": {},
   "source": [
    "- Summarize the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fb0730e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the second prompt template (summarize the review)\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2143d1",
   "metadata": {},
   "source": [
    "- Identify the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6accf92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the third prompt template (identify the language)\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0668e",
   "metadata": {},
   "source": [
    "- Write follow-up response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7a46121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the fourth prompt template (write follow-up response)\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b245a",
   "metadata": {},
   "source": [
    "- Creation of the SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89603117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a SequentialChain\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b04f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's strange. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer finds the taste of the product mediocre and questions if it is an old batch or counterfeit compared to ones purchased in stores.',\n",
       " 'followup_message': \"Bonjour,\\n\\nNous vous remercions pour votre retour concernant le goût du produit. Nous nous excusons si vous avez trouvé la qualité médiocre. Notre priorité est de garantir la fraîcheur et l'authenticité de nos produits. Nous allons enquêter pour vérifier s'il s'agit d'un lot ancien ou contrefait. Votre satisfaction est importante pour nous et nous ferons tout notre possible pour rectifier la situation. N'hésitez pas à nous contacter si vous avez d'autres préoccupations.\\n\\nCordialement, [Votre société]\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the chain\n",
    "review = df.Review[5]\n",
    "overall_chain.invoke(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc31d18",
   "metadata": {},
   "source": [
    "**Explanation output** The output contains a detailed analysis and response to a customer's review. The original review in French expresses dissatisfaction with the product's taste and questions its authenticity. This review is translated into English, summarized, and a follow-up message is provided to address the customer's concerns and ensure their satisfaction.  \n",
    "This structured approach helps in understanding customer feedback, translating it for broader accessibility, summarizing the key points for quick reference, and providing a professional and empathetic response to maintain customer relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041ea4c",
   "metadata": {},
   "source": [
    "### Router Chain\n",
    "This section demonstrates how to use Router Chain to route inputs to different chains based on the content of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade83f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define different prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f590e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a list of dictionaries containing information about 4 different subject prompts\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd934b",
   "metadata": {},
   "source": [
    "- Import required modules from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b06fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the MultiPromptChain class from the langchain.chains.router module\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "\n",
    "# Import the LLMRouterChain class and RouterOutputParser from the langchain.chains.router.llm_router module\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "\n",
    "# Import the PromptTemplate class from the langchain.prompts module\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f50bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of ChatOpenAI with a specified temperature and model\n",
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefec24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the destination chains\n",
    "destination_chains = {}\n",
    "\n",
    "# Loop through each prompt information dictionary in the prompt_infos list\n",
    "for p_info in prompt_infos:\n",
    "\n",
    "    # Extract the name and prompt template from the current prompt information dictionary\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    \n",
    "    # Create a ChatPromptTemplate instance using the extracted prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    \n",
    "    # Create an LLMChain instance with the initialized language model (llm) and the created prompt\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    # Add the created chain to the destination_chains dictionary with the name as the key\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98018a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the default prompt and chain\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2e2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the router template\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387109d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Format the router template with destinations\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "\n",
    "# Create the router prompt\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "# Create the router chain\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7d560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_13772\\3038952769.py:1: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(router_chain=router_chain,\n"
     ]
    }
   ],
   "source": [
    "# Create the MultiPromptChain\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d86b2131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_13772\\1733274077.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(\"What is black body radiation?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck's law. This radiation is characterized by a continuous spectrum of wavelengths and intensities, with the peak intensity shifting to shorter wavelengths as the temperature of the black body increases. Black body radiation plays a key role in understanding concepts such as thermal radiation and the quantization of energy in quantum mechanics.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b717379",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0c7a2",
   "metadata": {},
   "source": [
    "Issues in the above  Code  \n",
    "- Deprecation Warning: MultiPromptChain is deprecated; use RouterChain instead.  \n",
    "- Incorrect Prompt Formatting: You need to explicitly format MULTI_PROMPT_ROUTER_TEMPLATE with destinations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_13772\\4052776312.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0, model=llm_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is black body radiation?', 'text': \"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical body that absorbs all incident electromagnetic radiation and emits radiation at all frequencies. The radiation emitted by a black body is characterized by a continuous spectrum that depends only on the body's temperature, with the intensity of radiation increasing with increasing temperature. This phenomenon is described by Planck's law of black body radiation, which played a crucial role in the development of quantum mechanics.\"}\n"
     ]
    }
   ],
   "source": [
    "# Import required modules from the langchain library\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableBranch, RunnablePassthrough\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define LLM\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "# Define different prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor...\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician...\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian...\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "computerscience_template = \"\"\"You are a successful computer scientist...\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "# Define chains\n",
    "chains = {\n",
    "    \"physics\": LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(physics_template)),\n",
    "    \"math\": LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(math_template)),\n",
    "    \"history\": LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(history_template)),\n",
    "    \"computer science\": LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(computerscience_template)),\n",
    "}\n",
    "\n",
    "# Default chain (fallback)\n",
    "default_chain = LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(\"{input}\"))\n",
    "\n",
    "# Define routing function\n",
    "def route_logic(inputs):\n",
    "    input_text = inputs[\"input\"].lower()\n",
    "    if \"physics\" in input_text or \"radiation\" in input_text:\n",
    "        return \"physics\"\n",
    "    elif \"math\" in input_text or \"calculate\" in input_text:\n",
    "        return \"math\"\n",
    "    elif \"history\" in input_text:\n",
    "        return \"history\"\n",
    "    elif \"code\" in input_text or \"computer\" in input_text:\n",
    "        return \"computer science\"\n",
    "    return \"default\"\n",
    "\n",
    "# Use RunnableBranch for routing\n",
    "router = RunnableBranch(\n",
    "    (lambda x: route_logic(x) == \"physics\", chains[\"physics\"]),\n",
    "    (lambda x: route_logic(x) == \"math\", chains[\"math\"]),\n",
    "    (lambda x: route_logic(x) == \"history\", chains[\"history\"]),\n",
    "    (lambda x: route_logic(x) == \"computer science\", chains[\"computer science\"]),\n",
    "    default_chain  # Fallback case\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "response = router.invoke({\"input\": \"What is black body radiation?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77c46ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Why does every cell in our body contain DNA?', 'text': 'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of an organism. DNA contains the instructions for building and maintaining an organism, including the proteins that are essential for cell structure and function. This genetic information is passed down from parent to offspring and is essential for the growth, development, and functioning of all cells in the body. Having DNA in every cell ensures that the genetic information is preserved and can be used to carry out the necessary processes for life.'}\n"
     ]
    }
   ],
   "source": [
    "# Invoke the chain\n",
    "response = router.invoke({\"input\": \"Why does every cell in our body contain DNA?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae793e6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook provides an `overview of different chain mechanisms in LangChain`, demonstrating how they can be used to manage and orchestrate complex workflows involving language models. By leveraging these chain tools, developers can create more structured and contextually aware interactions with AI models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
