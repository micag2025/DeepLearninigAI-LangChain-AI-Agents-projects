{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16de7336",
   "metadata": {},
   "source": [
    "# Tagging and Extraction Using OpenAI functions üè∑Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34349d",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "This notebook demonstrates `how to perform tagging and extraction using OpenAI functions in combination with Langchain`. The main focus is on tagging text with specific attributes such as sentiment and language, and extracting structured information from text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fcbde",
   "metadata": {},
   "source": [
    "Sentiment analysis is a natural language processing (NLP) technique used to determine the sentiment or emotional tone behind a piece of text. This can be categorized as positive, negative, or neutral. Sentiment analysis is widely used in various applications such as social media monitoring, customer feedback analysis, and market research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a156354",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "First, we need to import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe58c35",
   "metadata": {},
   "source": [
    "### Example Using OpenAI and Langchain\n",
    "Here is a simplified example of how you can perform sentiment analysis using OpenAI and Langchain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb2ede9-b0b4-4c8c-b00f-9a9911f38614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules from Pydantic and Langchain\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1714bc8",
   "metadata": {},
   "source": [
    "- Define a Tagging model using Pydantic to specify the sentiment and language fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ae5481-4155-4831-a5dd-4c183b3b4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for Tagging\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e72cc",
   "metadata": {},
   "source": [
    "- Use the convert_pydantic_to_openai_function utility to convert this model into an OpenAI function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd656b2-62be-49d4-a277-b920c67203c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_10232\\1725091243.py:1: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  convert_pydantic_to_openai_function(Tagging)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Tagging',\n",
       " 'description': 'Tag the piece of text with particular info.',\n",
       " 'parameters': {'properties': {'sentiment': {'description': 'sentiment of text, should be `pos`, `neg`, or `neutral`',\n",
       "    'type': 'string'},\n",
       "   'language': {'description': 'language of text (should be ISO 639-1 code)',\n",
       "    'type': 'string'}},\n",
       "  'required': ['sentiment', 'language'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Pydantic model to OpenAI function\n",
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8b15a7-450c-43d6-af44-ca63800a4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for Langchain prompts and chat models\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61cafb",
   "metadata": {},
   "source": [
    "- Create a ChatOpenAI model and bind it with the tagging functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc5bac3-3783-4ce7-9de6-83c905fba7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michela\\AppData\\Local\\Temp\\ipykernel_10232\\1167291718.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(temperature=0)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798ac5b3-7e47-4cfb-8173-63900cf1e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tagging functions\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65372cc",
   "metadata": {},
   "source": [
    "- Finally, let's create a prompt template and a tagging chain to analyze the sentiment of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d558031-18cf-4791-b061-8911ce314605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for tagging\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00607aff-a64e-42b7-8cf4-893e8393dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the model with functions\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0bc519-2b8e-40d0-88ec-fc5ef0291f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tagging chain\n",
    "tagging_chain = prompt | model_with_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595b02a",
   "metadata": {},
   "source": [
    "-  Test the tagging chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8688fdba-0996-446c-a77b-318094944998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"pos\",\"language\":\"en\"}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 108, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fc273353-868a-4572-bb0a-cfd0291e9264-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tagging chain\n",
    "tagging_chain.invoke({\"input\": \"I love langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b4946",
   "metadata": {},
   "source": [
    "**Explanation output** The above output is an instance of an AIMessage, which encapsulates the response from an AI model with a given input (I love langchain). In summary, the AI model was called to tag a piece of text, identifying it as `having a positive sentiment and being in English`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6a9657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"neg\",\"language\":\"nl\"}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 113, 'total_tokens': 124, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c5656d48-92f2-471b-82ce-c3ddfb08c797-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tagging chain with a different input\n",
    "tagging_chain.invoke({\"input\": \"Ik houd niet van het Nederlandse weer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a69e9",
   "metadata": {},
   "source": [
    "**Explanation output** Using a different input, the above output has identified the input as `having a negative sentiment and being in Dutch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0749e408-3878-4ddd-ad33-8d5b8418a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JsonOutputFunctionsParser\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc45243e-7225-427d-b679-d737ebaec780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tagging chain with JSON output parser\n",
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbba085f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'neg', 'language': 'nl'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tagging chain with JSON output parser\n",
    "tagging_chain.invoke({\"input\": \"Ik houd niet van het Nederlandse weer\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e417b4-9306-413f-865f-e13dbd2d0196",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Extraction is similar to tagging, but used for extracting multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97338dce-a61a-4f8b-912c-6108dcb86183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for Person\n",
    "from typing import Optional\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f558fb45-f3a5-47be-8778-dddec2d00ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for Information\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eeb6028-0ade-46f6-a899-ca10f185bb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'properties': {'people': {'description': 'List of info about people',\n",
       "    'items': {'description': 'Information about a person.',\n",
       "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
       "      'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "       'description': \"person's age\"}},\n",
       "     'required': ['name', 'age'],\n",
       "     'type': 'object'},\n",
       "    'type': 'array'}},\n",
       "  'required': ['people'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Pydantic model to OpenAI function\n",
    "convert_pydantic_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f16ce50e-bad7-4fbb-b9e2-0adae6fc55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction functions\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f783ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Pinco\",\"age\":30},{\"name\":\"Palla\",\"age\":null}]}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 97, 'total_tokens': 120, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5535e87e-8190-4f1d-ac6e-321ee4d3ddc4-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the extraction model\n",
    "extraction_model.invoke(\"Pinco is 30, his mom is Palla\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "172df097-250a-4813-b76d-21436c13056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for extraction\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfec48d7-25a2-46d7-a7a2-33284b577dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extraction chain\n",
    "extraction_chain = prompt | extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19622af-4b01-4145-ae5a-af15b551a182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Pinco\",\"age\":30},{\"name\":\"Palla\",\"age\":null}]}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 114, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4a5993ac-a95d-448c-812b-f6f6e0fbee58-0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the extraction chain\n",
    "extraction_chain.invoke({\"input\": \"Pinco is 30, his mom is Palla\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38989431-9a61-461e-a86f-61a7dddab63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extraction chain with JSON output parser\n",
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ac89c9-1e8d-4e11-8bca-909a6c34d23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'name': 'Pinco', 'age': 30}, {'name': 'Palla', 'age': None}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the extraction chain with JSON output parser\n",
    "extraction_chain.invoke({\"input\": \"Pinco is 30, his mom is Palla\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8db4b39-c28b-4c0e-8b9a-55ceb8ba817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2901d3ac-4dfc-4fb9-afd3-ab056489d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7eff400-9f3a-42a8-ba4a-a3757e7939e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Pinco', 'age': 30}, {'name': 'Palla'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Pinco is 30, his mom is Palla\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8524f3-a663-4007-ad59-52dfe00343e9",
   "metadata": {},
   "source": [
    "## Doing it for real\n",
    "\n",
    "We can apply tagging to a larger body of text.\n",
    "\n",
    "For example, let's load this blog post and extract tag information from a sub-set of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc7c4a8c-d4c8-4400-b1d3-5c7c0c25786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load a document from the web using WebBaseLoader\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9d73e",
   "metadata": {},
   "source": [
    "**Note** The above output shows up a warninig of setting the USER_AGENT environment variable, you ensure that your requests are properly identified, which can improve compatibility and compliance with web servers' policies. However, in this case, the warning about the USER_AGENT environment variable not being set does not affect the functionality of the analysis in the provided file. It is simply a recommendation to provide better identification for your requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbd2e910-e0e0-447a-8118-6fe792f04e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the first document from the loaded documents\n",
    "doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5e39f78-ff18-4016-a751-7d0e4c82bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 10,000 characters of the document's content\n",
    "page_content = doc.page_content[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb8d98ee-e9b2-4ce8-982a-c06dc006da76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful gene\n"
     ]
    }
   ],
   "source": [
    "# Print the first 1,000 characters of the page content for a quick preview\n",
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db0d107-5284-4253-b769-dfedb97ce95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for Overview\n",
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bfe7fe6-a86d-416c-a7f2-373dc70fcba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pydantic model to an OpenAI function\n",
    "overview_tagging_function = [\n",
    "    convert_pydantic_to_openai_function(Overview)\n",
    "]\n",
    "\n",
    "# Bind the model with the tagging functions and specify the function call\n",
    "tagging_model = model.bind(\n",
    "    functions=overview_tagging_function,\n",
    "    function_call={\"name\":\"Overview\"}\n",
    ")\n",
    "\n",
    "# Create a tagging chain using the prompt, model, and JSON output parser\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "518f2337-a8eb-4eff-b764-b70e48545d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'The article discusses building autonomous agents powered by LLM (large language model) as the core controller, with components like planning, memory, and tool use. It also covers techniques like task decomposition, self-reflection, and Algorithm Distillation (AD) for improving agent performance.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, Algorithm Distillation (AD)'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tagging chain with the page content\n",
    "tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5be5d604-3d95-476a-afe4-b46b021534fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e56ca98-d05d-4199-a6ec-fe485b630da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_extraction_function = [\n",
    "    convert_pydantic_to_openai_function(Info)\n",
    "]\n",
    "extraction_model = model.bind(\n",
    "    functions=paper_extraction_function, \n",
    "    function_call={\"name\":\"Info\"}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f082b7a0-b7ea-488a-923c-aba8e4b185a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b931b",
   "metadata": {},
   "source": [
    "**Explanation output** Mention about the above output, that is not completly precise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a8dcce7-7032-49a8-893d-1659e2f51f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article follow by its author. \n",
    "\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5f4c8cc-d9a4-4556-a89e-94ec981c6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b63e00b6-0f06-4e74-948e-04805e66f40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT)', 'author': 'Wei et al. 2022'},\n",
       " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Liu et al. 2023', 'author': None},\n",
       " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
       " {'title': 'Chain of Hindsight (CoH)', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'Algorithm Distillation (AD)', 'author': 'Laskin et al. 2023'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "365bfa5b-f0d2-4a19-8db1-1b4a0f51703d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Paper A', 'author': 'Author A'},\n",
       " {'title': 'Paper B', 'author': 'Author B'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a3e18b5-0692-49dc-b589-5d4ae5a43fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a851b07c-0f73-4cf1-801f-9832190db93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbd6fd3c-8fca-4f16-a256-a66577b48eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f1309bf-16a5-43ef-aa8e-427cf5120938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5af35566-8f5f-4e02-86d2-c1a97ca4e573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b77be284-fca5-4ab0-ab36-b83b3ab53136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67bded5d-de85-488c-bd3f-d83acbcfea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04233d0a-fe44-45ed-9145-af3caaa86863",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa8c2f1e-f3ac-4d9c-8ded-541778a32ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'hi'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f7d0255-4b58-42c9-a747-44768a866633",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prep | extraction_chain.map() | flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dfa1f67-7246-4f4f-8967-4d7266df7b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'AutoGPT', 'author': None},\n",
       " {'title': 'GPT-Engineer', 'author': None},\n",
       " {'title': 'BabyAGI', 'author': None},\n",
       " {'title': 'Chain of thought', 'author': 'Wei et al. 2022'},\n",
       " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'LLM+P', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
       " {'title': 'Chain of Hindsight', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'Algorithm Distillation', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'Miller 1956', 'author': None},\n",
       " {'title': 'Duan et al. 2017', 'author': None},\n",
       " {'title': 'LSH: Locality-Sensitive Hashing', 'author': None},\n",
       " {'title': 'ANNOY: Approximate Nearest Neighbors Oh Yeah', 'author': None},\n",
       " {'title': 'HNSW: Hierarchical Navigable Small World', 'author': None},\n",
       " {'title': 'FAISS: Facebook AI Similarity Search', 'author': None},\n",
       " {'title': 'ScaNN: Scalable Nearest Neighbors', 'author': None},\n",
       " {'title': 'MRKL (Karpas et al. 2022)', 'author': None},\n",
       " {'title': 'TALM (Tool Augmented Language Models; Parisi et al. 2022)',\n",
       "  'author': None},\n",
       " {'title': 'Toolformer (Schick et al. 2023)', 'author': None},\n",
       " {'title': 'HuggingGPT (Shen et al. 2023)', 'author': None},\n",
       " {'title': 'API-Bank', 'author': 'Li et al. 2023'},\n",
       " {'title': 'ChemCrow', 'author': 'Bran et al. 2023'},\n",
       " {'title': 'Boiko et al. (2023)', 'author': 'null'},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. (2023)'},\n",
       " {'title': 'Park et al. 2023', 'author': None},\n",
       " {'title': 'Super Mario: The Classic Adventure', 'author': 'John Smith'},\n",
       " {'title': 'MVC Components in Python', 'author': 'Emily Johnson'},\n",
       " {'title': 'A Study on Machine Learning Algorithms', 'author': 'John Smith'},\n",
       " {'title': 'Deep Learning Techniques for Image Recognition',\n",
       "  'author': 'Emily Brown'},\n",
       " {'title': 'pytest', 'author': None},\n",
       " {'title': 'dataclasses', 'author': None},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'Webgpt: Browser-assisted question-answering with human feedback',\n",
       "  'author': 'Nakano et al.'},\n",
       " {'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools',\n",
       "  'author': 'Schick et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb460a-2b56-42be-acf6-4f61e40027f4",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "This notebook demonstrated how to use OpenAI functions in combination with Langchain to perform tagging and extraction on text. By defining Pydantic models and converting them to OpenAI functions, we were able to tag text with specific attributes and extract structured information. This approach can be extended to more complex use cases and integrated into larger applications. Besides, sentiment analysis is a powerful tool for understanding the emotional tone of text. By using OpenAI and Langchain, you can easily implement sentiment analysis in your applications to gain insights from textual data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
